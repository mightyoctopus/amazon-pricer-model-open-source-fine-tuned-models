{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0IPxhq+kv+tIoyQVEa11y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/amazon-pricer-model-open-source-fine-tuned-models/blob/main/w7_d3_sft_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with SFT Trainer"
      ],
      "metadata": {
        "id": "-L0os1dE4He7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sQSc2sSfTc2"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PmOw8duF4Rnu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.import_utils import BASE_FILE_REQUIREMENTS\n",
        "### Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"product-pricer\"\n",
        "HF_USER = \"MightyOctopus\"\n",
        "\n",
        "### Data\n",
        "\n",
        "# https://huggingface.co/datasets/MightyOctopus/amazon-pricer-dataset-v2-0\n",
        "DATASET_NAME = f\"{HF_USER}/amazon-pricer-dataset-v2-0\"\n",
        "MAX_SEQUENCE_LENGTH = 182\n",
        "\n",
        "### Run name for saving the model in the hub:\n",
        "RUN_NAME = f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "\n",
        "### Hyperparameters for QLoRA\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.1\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "\n",
        "### Hyperparameters for Training\n",
        "EPOCHS = 1 # more than 1 might be overkill\n",
        "BATCH_SIZE = 4 # Great for T4. If A100, this can be up to 16\n",
        "GRADIENT_ACCUMULATION_STEPS = 8 # Better generalization and stable learning\n",
        "LEARNING_RATE = 1e-4\n",
        "LR_SCHEDULER_TYPE = \"cosine\"\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "\n",
        "### Admin Config:\n",
        "STEPS = 50\n",
        "SAVE_STEPS = 2000\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "sw6oRb1F5HIF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUB_MODEL_NAME"
      ],
      "metadata": {
        "id": "it-JjEXM86t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "zKNJAMHb0rZE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Log in to Weights & Biases\n",
        "wandb_api_key = userdata.get(\"WANDB\")\n",
        "os.environ[\"WANDB\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "### Configure Weights & Biases dto record against the project\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ],
      "metadata": {
        "id": "V6JawiRL05vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ)"
      ],
      "metadata": {
        "id": "JTqMhPvf1ias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "I_0V0NHg3ZUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if wishing to reduce the training dataset to 20,000 points instead, then uncomment this line:\n",
        "# train = train.select(range(20000))"
      ],
      "metadata": {
        "id": "s_s05_uZBEaT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "    wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ],
      "metadata": {
        "id": "3ktZttl-BNuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Tokenizer and Model"
      ],
      "metadata": {
        "id": "P3vq2l7cBrc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Pick the right quantization config\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "else:\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )"
      ],
      "metadata": {
        "id": "n2IP9tpZBwBc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils import quantization_config\n",
        "### Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory Footprint: {base_model.get_memory_footprint() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "W8Oy7fiJC_Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator\n",
        "\n",
        "To ensure during Training that it is not trying to train the model to predict the description of products; only their prices -- specifically, after this line: \"Price is $\" in the train prompt\n",
        "\n",
        "This is a super simple helper class from Hugging Face to take care of it:"
      ],
      "metadata": {
        "id": "gtXnLHWsF1AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "response_template = \"Price is $\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "El1Kc2t3GV-W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Config & SFT Config"
      ],
      "metadata": {
        "id": "0wW4xJduI8lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES\n",
        ")\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=False\n",
        ")\n",
        "\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters,\n",
        "    data_collator=collator\n",
        ")"
      ],
      "metadata": {
        "id": "aKhV4ktsI-ac"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune"
      ],
      "metadata": {
        "id": "o3PpnSTFfEFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning.train()\n",
        "\n",
        "### Push the fine tuned model to HF hub\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME)"
      ],
      "metadata": {
        "id": "_IYSbdt0U-wn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}