{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTckUAFMlQaKFktQpAJWgz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/amazon-pricer-model-open-source-fine-tuned-models/blob/main/w7_d5_test_finetuned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ],
      "metadata": {
        "id": "sBVXflv3jS7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "un04f5hLjKPB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from peft import PeftModel\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"pricer\"\n",
        "ED_HF_USER = \"ed-donner\"\n",
        "MO_HF_USER = \"MightyOctopus\"\n",
        "\n",
        "RUN_NAME = \"2024-09-13_13.04.39\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "REVISION = \"e8d637df551603dc86cd7a1598a8f44af4d7ae36\"\n",
        "FINETUNED_MODEL = f\"{ED_HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "DATASET_NAME = f\"MightyOctopus/amazon-pricer-dataset-v2-0\"\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# Used for writing to output in color\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
      ],
      "metadata": {
        "id": "1jFbNRATuYOy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "8nTuOtxEv2Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "FV6P7CtIwO1i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[0]"
      ],
      "metadata": {
        "id": "7Ts6zb3xwysW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the tokenizer and models"
      ],
      "metadata": {
        "id": "Y-6d41rUw-RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Pick the right quantization config for preference (4 or 8 bit)\n",
        "if QUANT_4_BIT:\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "else:\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )"
      ],
      "metadata": {
        "id": "NuwE8wHKxEh3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "### Load the fine tuned model with PEFT\n",
        "if REVISION:\n",
        "    fine_tuned_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        FINETUNED_MODEL,\n",
        "        revision=REVISION\n",
        "    )\n",
        "else:\n",
        "    fine_tuned_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        FINETUNED_MODEL\n",
        "    )\n",
        "\n",
        "print(f\"Memory Footprint: {fine_tuned_model.get_memory_footprint() / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "ZXmS1ei3x9VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_price(s):\n",
        "    filter_phrase = \"Price is $\"\n",
        "    if filter_phrase in s:\n",
        "        content = s.split(filter_phrase)[1]\n",
        "        content = content.replace(\",\", \"\")\n",
        "        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", content)\n",
        "        return float(match.group()) if match else 0\n",
        "    return 0"
      ],
      "metadata": {
        "id": "poHFZAkL0kbO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_price(\"Price is $a fabulous 899.99 or so\")"
      ],
      "metadata": {
        "id": "4hi7lss928oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. take an input and tokenize it by cuda\n",
        "# 2. set attention mask on it\n",
        "# 3. generate output\n",
        "# 4. decode the output to the natural language\n",
        "def model_predict(prompt):\n",
        "    set_seed(42)\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
        "    outputs = fine_tuned_model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=3,\n",
        "        num_return_sequences=1\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0])\n",
        "\n",
        "    return extract_price(response)\n",
        ""
      ],
      "metadata": {
        "id": "T4FythCr4vHe"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}